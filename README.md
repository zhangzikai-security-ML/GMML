Multimodal learning integrates information from diverse modalities to enhance model robustness and generalization. However, real-world multimodal scenarios often suffer from heterogeneous and dynamic imbalance phenomena, such as noise interference, modality absence, and intermodal information disparities, which degrade model performance by inducing biased feature representations. 
Existing methods fail to adaptively modulate models under heterogeneous and dynamic imbalance conditions. To this end, we propose Gradient-Modulated Multimodal Learning (\texttt{GMML}), a robust framework that dynamically balances multimodal training gradients to counteract imbalance-induced biases. Specifically, i) To adaptively balance inter-modals, we design an imbalance-aware gradient modulation strategy that identifies contributions and achieves a smooth gradient weight transition to balance conflicting gradients during optimization; 
ii) To further address potential modality missing and single-modality noise, we propose a parameterr constraint based method that enforces an $\ell2$-norm constraint on encoder parameters, suppressing parameter space oscillations and blocking noisy gradient updates. 
To theoretically guarantee the performance and robustness of \texttt{GMML}, we prove that our method has a larger certified radius to adapt to complex modal imbalance cases, and a smaller convergence upper bound for faster adaptive capability.
Extensive experiments demonstrate GMMLâ€™s superior robustness against three imbalance types (noise, absence, and information disparities), outperforming state-of-the-art methods by 3.3\% and 2.3\% in accuracy on KS and UCF-101 benchmarks.
